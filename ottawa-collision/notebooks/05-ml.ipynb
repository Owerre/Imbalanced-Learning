{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Supervised Machine Learning\n",
    "\n",
    "In this notebook, we train the following supervised ML models: \n",
    "\n",
    "- Logistic Regression\n",
    "\n",
    "- Support Vector Machine classifier\n",
    "\n",
    "- Random Forest classifier\n",
    "\n",
    "- XGBoost classifier\n",
    "\n",
    "- SMOTE with XGBoost classifier\n",
    "\n",
    "The class distribution is 99.8 $\\%$ majority class and 0.2 $\\%$ minority class. The resulting  performance metrics of the training set show that none of these models predicted the minority class although the overall accuracy of the models are 99 $\\%$. This shows that accuracy is not a good performance measure for imbalanced datasets. \n",
    "\n",
    "The optimizimation of the area under the precision-recall (AUPR) curve and the area under the receiver operating characteristic (AUROC) curve does not give any improvement on the predictive power of the models, so we optimized the recall.  \n",
    "\n",
    "SMOTE in combination with XGBoost classifier was applied on the training set, but it did not give any improvement on the imbalanced test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Ignore deprecated warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set font scale and style\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Resampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Machine learning models\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Grid search and model selection\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model performance metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import  accuracy_score, auc,recall_score,precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\n",
    "\n",
    "# Pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom class\n",
    "%run -i '../src/helper/transfxn.py'\n",
    "%run -i '../src/helper/ml.py'\n",
    "%run -i '../src/helper/imputer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classes\n",
    "transfxn = TransformationPipeline()\n",
    "imputer = DataFrameImputer()\n",
    "model = SupervisedModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (89990, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>environment</th>\n",
       "      <th>light</th>\n",
       "      <th>surface_condition</th>\n",
       "      <th>traffic_control</th>\n",
       "      <th>traffic_control_condition</th>\n",
       "      <th>class</th>\n",
       "      <th>impact_type</th>\n",
       "      <th>no_of_pedestrians</th>\n",
       "      <th>...</th>\n",
       "      <th>hr_per_day</th>\n",
       "      <th>impact_per_hour</th>\n",
       "      <th>impact_per_day</th>\n",
       "      <th>envmt_per_hour</th>\n",
       "      <th>surcond_per_hour</th>\n",
       "      <th>enviro_ind</th>\n",
       "      <th>impact_ind</th>\n",
       "      <th>traf_cont_ind</th>\n",
       "      <th>sur_cont_ind</th>\n",
       "      <th>light_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>385914.80540</td>\n",
       "      <td>5.038118e+06</td>\n",
       "      <td>01 - Clear</td>\n",
       "      <td>05 - Dusk</td>\n",
       "      <td>05 - Packed snow</td>\n",
       "      <td>10 - No control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>06 - SMV unattended vehicle</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438425</td>\n",
       "      <td>1.563514</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>15.230470</td>\n",
       "      <td>0.520883</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>372695.25000</td>\n",
       "      <td>5.023408e+06</td>\n",
       "      <td>02 - Rain</td>\n",
       "      <td>07 - Dark</td>\n",
       "      <td>03 - Loose snow</td>\n",
       "      <td>10 - No control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>06 - SMV unattended vehicle</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192006</td>\n",
       "      <td>2.422058</td>\n",
       "      <td>0.465049</td>\n",
       "      <td>2.730808</td>\n",
       "      <td>1.965806</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356891.06250</td>\n",
       "      <td>5.006390e+06</td>\n",
       "      <td>01 - Clear</td>\n",
       "      <td>01 - Daylight</td>\n",
       "      <td>01 - Dry</td>\n",
       "      <td>01 - Traffic signal</td>\n",
       "      <td>01 - Functioning</td>\n",
       "      <td>0</td>\n",
       "      <td>04 - Sideswipe</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331986</td>\n",
       "      <td>2.948368</td>\n",
       "      <td>0.978816</td>\n",
       "      <td>17.140770</td>\n",
       "      <td>14.264247</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>359589.46875</td>\n",
       "      <td>5.019488e+06</td>\n",
       "      <td>01 - Clear</td>\n",
       "      <td>01 - Daylight</td>\n",
       "      <td>01 - Dry</td>\n",
       "      <td>10 - No control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>03 - Rear end</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445636</td>\n",
       "      <td>6.483926</td>\n",
       "      <td>2.889469</td>\n",
       "      <td>14.984032</td>\n",
       "      <td>12.469449</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>378187.65625</td>\n",
       "      <td>5.014852e+06</td>\n",
       "      <td>01 - Clear</td>\n",
       "      <td>07 - Dark</td>\n",
       "      <td>01 - Dry</td>\n",
       "      <td>10 - No control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>03 - Rear end</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254290</td>\n",
       "      <td>7.966257</td>\n",
       "      <td>2.025742</td>\n",
       "      <td>18.409626</td>\n",
       "      <td>15.320167</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x             y environment          light surface_condition  \\\n",
       "0  385914.80540  5.038118e+06  01 - Clear      05 - Dusk  05 - Packed snow   \n",
       "1  372695.25000  5.023408e+06   02 - Rain      07 - Dark   03 - Loose snow   \n",
       "2  356891.06250  5.006390e+06  01 - Clear  01 - Daylight          01 - Dry   \n",
       "3  359589.46875  5.019488e+06  01 - Clear  01 - Daylight          01 - Dry   \n",
       "4  378187.65625  5.014852e+06  01 - Clear      07 - Dark          01 - Dry   \n",
       "\n",
       "       traffic_control traffic_control_condition  class  \\\n",
       "0      10 - No control                       NaN      0   \n",
       "1      10 - No control                       NaN      0   \n",
       "2  01 - Traffic signal          01 - Functioning      0   \n",
       "3      10 - No control                       NaN      0   \n",
       "4      10 - No control                       NaN      0   \n",
       "\n",
       "                   impact_type  no_of_pedestrians  ...  hr_per_day  \\\n",
       "0  06 - SMV unattended vehicle                  0  ...    0.438425   \n",
       "1  06 - SMV unattended vehicle                  0  ...    0.192006   \n",
       "2               04 - Sideswipe                  0  ...    0.331986   \n",
       "3                03 - Rear end                  0  ...    0.445636   \n",
       "4                03 - Rear end                  0  ...    0.254290   \n",
       "\n",
       "  impact_per_hour impact_per_day  envmt_per_hour  surcond_per_hour  \\\n",
       "0        1.563514       0.685484       15.230470          0.520883   \n",
       "1        2.422058       0.465049        2.730808          1.965806   \n",
       "2        2.948368       0.978816       17.140770         14.264247   \n",
       "3        6.483926       2.889469       14.984032         12.469449   \n",
       "4        7.966257       2.025742       18.409626         15.320167   \n",
       "\n",
       "   enviro_ind  impact_ind  traf_cont_ind  sur_cont_ind  light_ind  \n",
       "0           N           N              N             N          N  \n",
       "1           N           N              N             N          Y  \n",
       "2           N           N              N             N          N  \n",
       "3           N           N              N             N          N  \n",
       "4           N           N              N             N          Y  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/feat_engr_data.csv') # Load cleaned data\n",
    "\n",
    "df = df.sample(frac =1).reset_index(drop = True) # shuffle\n",
    "\n",
    "print('Data size:', df.shape) # data size\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>89846</td>\n",
       "      <td>99.839982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0.160018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels  count  percentage\n",
       "0       0  89846   99.839982\n",
       "1       1    144    0.160018"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pct = df['class'].value_counts(normalize = True)*100\n",
    "label_ct =  df['class'].value_counts()\n",
    "pd.DataFrame({'labels': label_pct.index, 'count': label_ct.values, 'percentage': label_pct.values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix and class label\n",
    "X, y = df.drop('class', axis = 1), df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test set\n",
    "We now split the data set into 80$\\%$ training set and 20$\\%$ test set in a stratify fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (71992, 30) (71992,)\n",
      "Test set size: (17998, 30) (17998,)\n"
     ]
    }
   ],
   "source": [
    "print('Training set size:', X_train.shape, y_train.shape)\n",
    "print('Test set size:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      " 0    99.84026\n",
      "1     0.15974\n",
      "Name: class, dtype: float64\n",
      "------------------------------\n",
      "Test set class distribution:\n",
      " 0    99.838871\n",
      "1     0.161129\n",
      "Name: class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Training set class distribution:\\n', (y_train.value_counts()/X_train.shape[0])*100)\n",
    "print('--' * 15)\n",
    "print('Test set class distribution:\\n', (y_test.value_counts()/X_test.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit transform the training set\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_imputed = imputer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and scale data\n",
    "X_train_scaled, X_test_scaled, feat_names = transfxn.preprocessing(X_train_imputed, X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size after pre-processing: (71992, 101)\n",
      "Test set size after pre-processing: (17998, 101)\n"
     ]
    }
   ],
   "source": [
    "# Size of the data after pre-processing\n",
    "print('Training set size after pre-processing:', X_train_scaled.shape)\n",
    "print('Test set size after pre-processing:', X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the class labels to arrays\n",
    "y_train, y_test = y_train.values,  y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Model Selection by Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-1. Logistic Regression\n",
    "Logistic regression predicted only one class - the majority class, although the accuracy and AUROC are very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold cross-validation results for Logistic Regression\n",
      "------------------------------------------------------------\n",
      "Accuracy (std): 0.998375 (0.000034)\n",
      "AUROC: 0.837079\n",
      "AUPRC: 0.011036\n",
      "Predicted classes: [0 1]\n",
      "Confusion matrix:\n",
      " [[71875     2]\n",
      " [  115     0]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71877\n",
      "           1       0.00      0.00      0.00       115\n",
      "\n",
      "    accuracy                           1.00     71992\n",
      "   macro avg       0.50      0.50      0.50     71992\n",
      "weighted avg       1.00      1.00      1.00     71992\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression()   \n",
    "model.eval_metrics_cv(log_clf, X_train_scaled, y_train, cv_fold = 5, scoring = 'accuracy',\n",
    "                      model_nm = \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625.0173913043478"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class ratio of the negative class \n",
    "# to the positive class\n",
    "neg = y_train == 0\n",
    "pos = y_train == 1\n",
    "class_ratio = np.sum(neg)/np.sum(pos)\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Grid search best AUC score: 0.8387711007949609\n",
      "Grid search best parameters: {'class_weight': {0: 1, 1: 3}, 'C': 0.25}\n",
      "\n",
      "5-Fold cross-validation results for Logistic Regression with best hyperparameters\n",
      "------------------------------------------------------------\n",
      "Accuracy (std): 0.998361 (0.000056)\n",
      "AUROC: 0.837162\n",
      "AUPRC: 0.011555\n",
      "Predicted classes: [0 1]\n",
      "Confusion matrix:\n",
      " [[71874     3]\n",
      " [  115     0]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71877\n",
      "           1       0.00      0.00      0.00       115\n",
      "\n",
      "    accuracy                           1.00     71992\n",
      "   macro avg       0.50      0.50      0.50     71992\n",
      "weighted avg       1.00      1.00      1.00     71992\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Range of hyperparameters\n",
    "params = {'C': [2**x for x in range(-2,9,2)], \n",
    "          'class_weight': ['balanced', {0:1, 1:3}, {0:1, 1:class_ratio}]\n",
    "          }\n",
    "                             \n",
    "# Grid search\n",
    "gsearch_log = RandomizedSearchCV(estimator = log_clf, param_distributions = params, \n",
    "                                scoring = 'roc_auc', cv = 5, n_jobs = -1, \n",
    "                                 n_iter = 200,random_state = 42, verbose = 1)\n",
    "\n",
    "# Fit the  training set\n",
    "gsearch_log.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Pickle trained model\n",
    "joblib.dump(gsearch_log.best_estimator_, '../src/model/log_clf.pkl')\n",
    "\n",
    "# Print results\n",
    "print('Grid search best AUC score:', gsearch_log.best_score_)\n",
    "print('Grid search best parameters:', gsearch_log.best_params_)\n",
    "print()\n",
    "model.eval_metrics_cv(gsearch_log.best_estimator_, X_train_scaled, y_train, cv_fold = 5,\n",
    " scoring = 'accuracy', model_nm = \"Logistic Regression with best hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold cross-validation results for SVM Classifier\n",
      "------------------------------------------------------------\n",
      "Accuracy (std): 0.998403 (0.000000)\n",
      "AUROC: 0.691234\n",
      "AUPRC: 0.009473\n",
      "Predicted classes: [0]\n",
      "Confusion matrix:\n",
      " [[71877     0]\n",
      " [  115     0]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71877\n",
      "           1       0.00      0.00      0.00       115\n",
      "\n",
      "    accuracy                           1.00     71992\n",
      "   macro avg       0.50      0.50      0.50     71992\n",
      "weighted avg       1.00      1.00      1.00     71992\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(probability = True, kernel = 'rbf')   \n",
    "model.eval_metrics_cv(svm_clf, X_train_scaled, y_train, cv_fold = 5, scoring = 'accuracy', \n",
    "                      model_nm = \"SVM Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n"
     ]
    }
   ],
   "source": [
    "# Range of hyperparameters\n",
    "params = {'C': [2**x for x in range(-2,11,2)], \n",
    "          'gamma': [2**x for x in range(-11,1,2)],\n",
    "          'class_weight': [None, 'balanced',{0:1, 1:class_ratio}]\n",
    "         } \n",
    "                                                              \n",
    "# Randomized search for SVM\n",
    "svm_clf = SVC(probability = True, kernel = 'rbf')\n",
    "rsearch_svm = RandomizedSearchCV(svm_clf, param_distributions = params, cv = 5,\n",
    "                                 scoring = 'roc_auc', n_iter =200,\n",
    "                                 n_jobs = -1,random_state = 42, verbose = 1) \n",
    "# Fit the training set\n",
    "rsearch_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Pickle trained model\n",
    "joblib.dump(rsearch_svm.best_estimator_, '../src/model/svm_clf.pkl')\n",
    "\n",
    "print('Best recall AUC score: ', rsearch_svm.best_score_)\n",
    "print('Best parameters: ', rsearch_svm.best_params_)\n",
    "print()\n",
    "model.eval_metrics_cv(rsearch_svm.best_estimator_, X_train_scaled, y_train, cv_fold = 5,\n",
    " scoring = 'accuracy', model_nm = \"SVM Classifier with Best Hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state = 42)                         \n",
    "model.eval_metrics_cv(rf_clf, X_train_scaled, y_train, cv_fold = 5, scoring = 'accuracy', \n",
    "                      model_nm = \"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importances\n",
    "importances_df = pd.DataFrame({'Features': feat_names, \n",
    "                                'Importances': rf_clf.feature_importances_\n",
    "                              })\n",
    "# Bar plot\n",
    "importances_df.sort_values('Importances', ascending = True, inplace = True)\n",
    "importances_df.set_index('Features', inplace = True)\n",
    "importances_df.tail(20).plot(kind='barh', figsize = (18,10))\n",
    "plt.title('Top 20 Feature Importances for Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A-4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'objective':'binary:logistic', 'eval_metric':'logloss', 'random_state':42}\n",
    "\n",
    "xgb_clf = XGBClassifier(**param_dist) \n",
    "model.eval_metrics_cv(xgb_clf, X_train_scaled, y_train, cv_fold = 5, scoring = 'accuracy', \n",
    "                      model_nm = \"XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importances\n",
    "importances_df = pd.DataFrame({'Features': feat_names,\n",
    "                                 'Importances': xgb_clf.feature_importances_\n",
    "                              })\n",
    "# Bar plot\n",
    "importances_df.sort_values('Importances', ascending = True, inplace = True)\n",
    "importances_df.set_index('Features', inplace = True)\n",
    "importances_df.tail(20).plot(kind='barh', figsize = (18,10))\n",
    "plt.title('Top 20 Feature Importances for XGBoost Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Hyperparameter tuning - XGB\n",
    "Based on the results above, we select XGBoost and tune the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class ratio of the negative class to the positive class\n",
    "neg = y_train == 0\n",
    "pos = y_train == 1\n",
    "class_ratio = np.sum(neg)/np.sum(pos)\n",
    "\n",
    "# Range of hyperparameters\n",
    "params = {'subsample':[i/10 for i in range(5,9)],\n",
    "            'colsample_bytree': [i/10 for i in range(5,9)]\n",
    "            }\n",
    "\n",
    "# Randomized search\n",
    "param_dist = {'objective':'binary:logistic', 'eval_metric':'logloss', \n",
    "                'n_estimators':1000,'scale_pos_weight':class_ratio, \n",
    "                'learning_rate':0.1, 'min_child_weight':5, \n",
    "                'max_depth':9,'random_state':42\n",
    "             }\n",
    "              \n",
    "xgb_clf = XGBClassifier(**param_dist)\n",
    "rsearch_xgb = RandomizedSearchCV(estimator = xgb_clf, param_distributions = params, \n",
    "                                  scoring = 'roc_auc', cv = 5, n_jobs = -1, n_iter = 200, \n",
    "                                  random_state = 42, verbose = 1)   \n",
    "# Fit the  training set                                                            \n",
    "rsearch_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Pickle trained model\n",
    "joblib.dump(rsearch_xgb.best_estimator_, '../src/model/xgb_clf.pkl')\n",
    "\n",
    "# Print results\n",
    "print('Randomized search best AUC score:', rsearch_xgb.best_score_) \n",
    "print('Randomized search best hyperparameters:', rsearch_xgb.best_params_) \n",
    "print()\n",
    "model.eval_metrics_cv(rsearch_xgb.best_estimator_, X_train_scaled, y_train, cv_fold = 5,\n",
    " scoring = 'accuracy', model_nm = \"XGBoost Classifier with best hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Resampling Methods\n",
    "In this section, we will employ a resampling technique on the training set to balance the classes. However, the final prediction will be made on the imbalanced test set. The idea of resampling is to trick the classifier using a balanced dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C-1. SMOTE  combined with XGBoost Classifier\n",
    "In Synthetic Minority Over Sampling Technique (SMOTE), we generate synthetic oberservations to match the minority clas.\n",
    "SMOTE oversampled the minority class in the training set, so we now have equal class distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over sample the minority class\n",
    "sm = SMOTE(random_state = 42)\n",
    "X_train_scaled_ovsm, y_train_ovsm = sm.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SMOTE training data size:', X_train_scaled_ovsm.shape, y_train_ovsm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Imbalanced training set class distribution:', np.bincount(y_train))\n",
    "print('SMOTE resampled training set class distribution:', np.bincount(y_train_ovsm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost cross-validation on the SMOTE dataset\n",
    "param_dist = {'objective':'binary:logistic', 'eval_metric':'logloss', 'learning_rate':0.1,\n",
    "            'random_state':42\n",
    "            }\n",
    "            \n",
    "xgb_ovsm  = XGBClassifier(**param_dist)\n",
    "model.eval_metrics_cv(xgb_ovsm, X_train_scaled_ovsm, y_train_ovsm, cv_fold = 5, \n",
    "                      scoring = 'accuracy', model_nm = \"SMOTE with XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "xgb_clf = joblib.load('../src/model/xgb_clf.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Prediction on the Imbalanced Test Set \n",
    "In this section, we make our final prediction on the imbalanced dataset after training the model using the resampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-1. Normal Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_pred(xgb_clf, X_train_scaled, y_train, X_test_scaled, y_test, model_nm = \"XBoost Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-2. SMOTE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_pred(xgb_ovsm, X_train_scaled_ovsm, y_train_ovsm, X_test_scaled, y_test, model_nm = \"SMOTE with XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E.  ROC and PR Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,15))\n",
    "\n",
    "# Normal imbalanced distribution\n",
    "model.plot_roc_pr_curves(xgb_clf, X_train_scaled, y_train, X_test_scaled, y_test, cv_fold = 5,\n",
    "                       color= 'b', label = 'Normal (AUC= %0.2f)')\n",
    "                     \n",
    "# SMOTE distribution\n",
    "model.plot_roc_pr_curves(xgb_ovsm, X_train_scaled_ovsm, y_train_ovsm, X_test_scaled, y_test, \n",
    "                         cv_fold = 5, color= 'r', label = 'SMOTE (AUC= %0.2f)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
